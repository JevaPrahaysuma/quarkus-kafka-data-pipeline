# ğŸ“Œ Data Pipeline Project

## ğŸ“– Overview
This project is a **simple data pipeline** built with **Quarkus, PostgreSQL, and Apache Kafka**.  
It demonstrates how incoming messages from Kafka are **processed**, **persisted** into the database, and **exposed via a REST API**.

The project follows a **modular clean architecture**:

- ğŸ“¨ **Consumer** â†’ Listens to Kafka topic  
- âš™ï¸ **Service** â†’ Processes messages  
- ğŸ’¾ **Repository** â†’ Handles persistence  
- ğŸŒ **Controller** â†’ REST API for accessing data  

---

## ğŸ—ï¸ System Design

### ğŸ–¼ï¸ Architecture Diagram
```mermaid
flowchart LR
    Kafka[(Kafka Topic)]
    Consumer[KafkaConsumerService]
    Service[DataProcessingService]
    Repo[DataRepository]
    DB[(PostgreSQL Database)]
    API[DataController]

    Kafka --> Consumer --> Service --> Repo --> DB
    API --> Repo
```

### ğŸ”„ Sequence Diagram
```mermaid
sequenceDiagram
    participant P as Producer (Kafka)
    participant C as KafkaConsumerService
    participant S as DataProcessingService
    participant R as DataRepository
    participant DB as PostgreSQL
    participant API as DataController

    P->>C: Publish message
    C->>S: consume(message)
    S->>R: processAndSave(message)
    R->>DB: INSERT DataEntity
    API->>R: GET /data
    R->>API: return List<DataEntity>
```

---

## ğŸ“‚ Project Structure
```
quarkus-kafka-data-pipeline/
 â”£ src/main/java/com/example/pipeline/
 â”ƒ â”£ controller/       # REST API Layer
 â”ƒ â”£ consumer/         # Kafka Consumer
 â”ƒ â”£ model/            # JPA Entities
 â”ƒ â”£ repository/       # Panache Repository
 â”ƒ â”— service/          # Business Logic
 â”£ src/test/java/com/example/pipeline/
 â”ƒ â”— DataPipelineTest.java
 â”£ src/main/resources/
 â”ƒ â”— application.properties
 â”— README.md
```

---

## âš¡ API Documentation

### 1ï¸âƒ£ Get All Data  
**Endpoint:** `GET /data`  
**Description:** Retrieves all data from database.  

ğŸ“¥ **Response Example**:
```json
[
  {
    "id": 1,
    "name": "KafkaMessage",
    "value": 123456789
  }
]
```

---

### 2ï¸âƒ£ Create Data  
**Endpoint:** `POST /data`  
**Description:** Creates a new DataEntity entry.  

ğŸ“¤ **Request Example**:
```json
{
  "name": "ManualEntry",
  "value": 42
}
```

ğŸ“¥ **Response Example**:
```json
{
  "id": 2,
  "name": "ManualEntry",
  "value": 42
}
```

---

## â–¶ï¸ Running the Project

### 1. Start PostgreSQL
Make sure PostgreSQL is running and configured in `application.properties`:

```properties
quarkus.datasource.db-kind=postgresql
quarkus.datasource.username=postgres
quarkus.datasource.password=postgres
quarkus.datasource.jdbc.url=jdbc:postgresql://localhost:5432/pipeline
quarkus.hibernate-orm.database.generation=update
```

### 2. Start Kafka (Local)
- âœ… If using **Dev Services (Quarkus)** â†’ Kafka will run automatically in dev mode.  
- âš¡ Or manually start **Kafka + Zookeeper** (e.g., via Docker).

### 3. Run Application
```bash
mvn quarkus:dev
```

### 4. Access API
Open in browser: [http://localhost:8080/data](http://localhost:8080/data)

---

## âœ… Testing

Run unit tests:
```bash
mvn test
```

Example: **DataPipelineTest.java**
- Saves a test message using `DataProcessingService`
- Verifies it was persisted correctly in the database  

---

## ğŸš€ Features
- ğŸ“¨ Kafka consumer integrated with **Quarkus Reactive Messaging**  
- ğŸ’¾ PostgreSQL persistence with **Panache ORM**  
- ğŸŒ REST API for data access  
- ğŸ§ª Unit test with **QuarkusTest**  

---

## ğŸ“Œ Next Steps
- ğŸ”— Add integration test for Kafka consumer  
- ğŸ³ Add `docker-compose` for easier local setup (**Kafka + Postgres**)  
- âš ï¸ Implement error handling & retries  